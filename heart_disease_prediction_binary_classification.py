# -*- coding: utf-8 -*-
"""Heart disease prediction - Binary Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/tejakanchi/heart-disease-prediction-binary-classification.ad1b7ea7-a7c4-45fb-bc72-8ab5d874984f.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250829/auto/storage/goog4_request%26X-Goog-Date%3D20250829T141003Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D28091b68ac049df54de6a1862643a995ba4d9659231973b82868811783c39cbd0376ee854d43325d42ab83df57330f02a63b8f21ffb7f0917a8843bfba41bdfcb0cc0dd9fc4f2f2b8651f30937e588d99339152f775a1fbb15c77eae9ed04ce1b3c7ce955e0732bcb9b430afe98acfa8a2da75992244f2e0e0c41ccc5f71620d75170619b9be67477c0f99ffb73c15c1895b7b943d3ed7a8832f4b1b8aceac0f7a532847dedc9342d8a8045103e094515457c50a49ef4939032a1fadc44f0780a6cbf8d78321852a1795207a0c6f78bfae054bfe887249179f5eb60ced530be37ae880eb8f38ef8e51084e7b0b915b76179cfdfbe35bd753bf8eeb6cdbd3c322
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
redwankarimsony_heart_disease_data_path = kagglehub.dataset_download('redwankarimsony/heart-disease-data')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

from sklearn.metrics import classification_report, confusion_matrix

"""# Reading Data"""

df = pd.read_csv("/kaggle/input/heart-disease-data/heart_disease_uci.csv")
y = df['num']
X = df.drop('num', axis = 1)

# As the number of rows are very less ,
# converting this problem into binary classification

df['num'] = df['num'].replace(2,1)
df['num'] = df['num'].replace(3,1)
df['num'] = df['num'].replace(4,1)

y = df['num']

y.value_counts()

X.columns

X.shape

y.shape

X.head()

X.info()

int_col = X.select_dtypes(include = ['int32', 'int64', 'float64'])

int_col

cat_col =  X.select_dtypes(include = ['object']).columns
print(cat_col)

# check nulls in dataset
X.isnull().sum()

"""# Check Data distribution to determine best way of imputing missing values"""

import matplotlib.pyplot as plt
import seaborn as sns

for col in X.columns:
    if col in int_col:
        plt.figure(figsize = (5,6))
        sns.histplot(X[col], kde = True, bins = 10)
        plt.xlabel(col)
        plt.show()

#trestbps, chol, oldpeak, thalch all are almost normal distributed, can be mode imputed nulls,
# as missing values are very few

"""The issue is with how you're using .mode()—it returns a Series, not a single value. You need to extract the first value from that Series before using it in fillna.
there can be more than 1 mode
"""

X['trestbps'].fillna(X['trestbps'].mode()[0], inplace = True)
X['chol'].fillna(X['chol'].mode()[0], inplace = True)
X['oldpeak'].fillna(X['oldpeak'].mode()[0], inplace = True)
X['thalch'].fillna(X['thalch'].mode()[0], inplace = True)

# fill missing cells with mode as only < 100 cells missing
# sex,fbs, exang
X['fbs'].fillna(X['fbs'].mode()[0], inplace = True)
X['exang'].fillna(X['exang'].mode()[0], inplace = True)
X['restecg'].fillna(X['restecg'].mode()[0], inplace = True)

X['slope'] = X['slope'].fillna('unknown')
X['thal'] = X['thal'].fillna('unknown')
X['ca'] = X['ca'].replace(np.nan, -1)

X.isnull().sum()

X.drop('id', axis =1, inplace = True)

# Handling boolean columns
X['sex'] = X['sex'].replace('Male', 1)
X['sex'] = X['sex'].replace('Female', 0)

X['fbs'] = X['fbs'].replace(True, 1)
X['fbs'] = X['fbs'].replace(False, 0)

X['exang'] = X['exang'].replace(True, 1)
X['exang'] = X['exang'].replace(False, 0)

X.isnull().sum()

X.head()

"""Natural order:
Medically and in the dataset, the risk/severity interpretation is roughly:
* Slope
Upsloping (least risky / normal response)

Flat (intermediate)

Downsloping (most indicative of ischemia / higher risk)

Unknown → usually treated as missing (NaN)

upsloping → 0
flat      → 1
downsloping → 2
unknown  → NaN or -1

* restecg
0=normal
1=ST-T wave abnormality
2=left ventricular hypertrophy

# thal

* 0 normal <

* 1 fixed defect

* 2 reversible defect

* -1 unknown

# Ordinal Encoding of thal, cp, slope, restecg
"""

# All mapping is done based on domain knowledge
slope_mapping = {'downsloping': 2, 'flat' :1, 'upsloping': 0, 'unknown' : -1 }
thal_mapping = {'normal' : 0, 'fixed defect' : 1, 'reversable defect' : 2, 'unknown' : -1 }
cp_mapping = {'typical angina': 0, 'atypical angina' : 1, 'non-anginal': 2, 'asymptomatic': 3}
restecg_mapping = {'normal' :0,'st-t abnormality': 1, 'lv hypertrophy': 2}



X['slope_encoded'] = X['slope'].map(slope_mapping)
X['thal_encoded'] = X['thal'].map(thal_mapping)
X['cp_encoded'] = X['cp'].map(cp_mapping)
X['restecg_encoded'] = X['restecg'].map(restecg_mapping)


X.drop('slope', axis =1 , inplace = True)
X.drop('thal', axis =1 , inplace = True)
X.drop('cp', axis =1 , inplace = True)
X.drop('restecg', axis =1 , inplace = True)

X.columns

#One hot encode - dataset
dummies = pd.get_dummies(X['dataset'], drop_first = True, dtype = int)
X = pd.concat([X.drop('dataset', axis = 1), dummies], axis = 1)

X.dtypes

X.isnull().sum()

X.shape

# Train-test-split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

"""# XGBoost classifier"""

from xgboost import XGBClassifier
from scipy.stats import uniform, randint
from sklearn.model_selection import RandomizedSearchCV


XGB_classifier = XGBClassifier(class_weight='balanced', random_state = 42)

param_dist = {
 'n_estimators': randint(100, 500),
    'max_depth': randint(3, 15),
    'learning_rate': uniform(0.01, 0.3),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'gamma': uniform(0, 5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}


random_search = RandomizedSearchCV(param_distributions = param_dist,
                                   estimator = XGB_classifier, cv = 3, n_iter=20,random_state=42,
                                  scoring = 'recall')

random_search.fit(X_train, y_train)
best_model = random_search.best_estimator_

y_pred = best_model.predict(X_test)

print("\nClassification Report for XGBClassifier :")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix for XGBClassifier:")
print(confusion_matrix(y_test, y_pred))

"""# Adjusting Class imbalance - telling model class 1 is more important"""

from xgboost import XGBClassifier
from scipy.stats import uniform, randint
from sklearn.model_selection import RandomizedSearchCV

scale_pos_weight = len(y_train[y_train==0])/ len(y_train[y_train==1])

XGB_classifier = XGBClassifier(scale_pos_weight=scale_pos_weight, eval_metric='logloss',random_state = 42)

param_dist = {
 'n_estimators': randint(100, 500),
    'max_depth': randint(3, 15),
    'learning_rate': uniform(0.01, 0.3),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'gamma': uniform(0, 5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}


random_search = RandomizedSearchCV(param_distributions = param_dist,
                                   estimator = XGB_classifier, cv = 3, n_iter=20,random_state=42,
                                  scoring = 'recall')

random_search.fit(X_train, y_train)
best_model = random_search.best_estimator_

y_pred = best_model.predict(X_test)

print("\nClassification Report for XGBClassifier :")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix for XGBClassifier:")
print(confusion_matrix(y_test, y_pred))

"""# Adding interaction features for improving recall"""

import pandas as pd
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix

# Binary target: 0 = no disease, 1 = any disease (num 1-4)
y = y.apply(lambda x: 1 if x > 0 else 0)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# Create interaction features
X_train['cp_slope'] = X_train['cp_encoded'] * X_train['slope_encoded']
X_test['cp_slope']  = X_test['cp_encoded'] * X_test['slope_encoded']

X_train['oldpeak_high'] = (X_train['oldpeak'] > 1.0).astype(int)
X_test['oldpeak_high']  = (X_test['oldpeak'] > 1.0).astype(int)

model = XGBClassifier(
    max_depth=3,
    min_child_weight=1,
    n_estimators=150,
    learning_rate=0.05,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)
model.fit(X_train, y_train)

# Predict probabilities
y_pred_proba = model.predict_proba(X_test)[:,1]

#  Adjust threshold to increase recall
threshold = 0.45
y_pred = (y_pred_proba >= threshold).astype(int)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

"""# Summary
* Recall for class 1 - diseased class has improved significantly from 0.80 to 0.88 which is basically meaning that the model predicts diseased people better
* Recall for class 0 got a bit low from 0.84 to 0.78 , but that is far less risky as compared to class 1
"""



                                 <<<<<<<<<<<README.MD>>>>>>>>>>>>>>>>>>>>>>>





"""# Task
Push the current notebook to the GitHub repository "https://github.com/Kanchimunichenchureddy/UCI-Heart-Disease-Data.git".

## Install git

### Subtask:
Install git in the Colab environment.

**Reasoning**:
Install git by updating the package list and then installing git.
"""

!apt-get update
!apt-get install git

"""## Clone the repository

### Subtask:
Clone the GitHub repository to the Colab environment.

**Reasoning**:
Clone the specified GitHub repository to the current environment.
"""

!git clone https://github.com/Kanchimunichenchureddy/UCI-Heart-Disease-Data.git

"""## Copy the notebook

### Subtask:
Copy the notebook file to the cloned repository directory.

**Reasoning**:
Copy the current notebook file to the cloned repository directory.
"""

!cp /content/notebook.ipynb /content/UCI-Heart-Disease-Data/

"""**Reasoning**:
The previous attempt to copy the notebook failed because the file path was incorrect. I need to find the correct path of the notebook file and then copy it to the cloned repository directory. I will use shell commands to find the file.


"""

!pwd
!ls /content/

"""**Reasoning**:
The output shows that the notebook file is not directly in the `/content/` directory. I need to find the notebook file within the Colab environment, which is typically located in `/content/`. Since `ls /content/` only shows `sample_data` and `UCI-Heart-Disease-Data`, the notebook file must be named something else or located elsewhere. I will try listing all files in `/content/` including hidden ones and recursively search for ipynb files to confirm the notebook's location.


"""

!ls -a /content/
!find /content/ -name "*.ipynb"

"""## Copy the notebook

### Subtask:
Copy the notebook file to the cloned repository directory.

**Reasoning**:
Search the Colab environment's file system for the location of the current notebook file by listing files in the root directory and recursively searching for ipynb files in the current and parent directories.
"""

!ls /
!find / -name "*.ipynb"

"""## Add and commit changes

### Subtask:
Add the notebook file to the git repository and commit the changes.

## Push to github

### Subtask:
Push the committed changes to the remote GitHub repository.

**Reasoning**:
Change the directory to the cloned repository, configure git with username and email, and then push the committed changes to the remote repository.
"""

import os
os.chdir('/content/UCI-Heart-Disease-Data')
!git config user.name "Your Name"
!git config user.email "your.email@example.com"
!git push origin main

"""**Reasoning**:
The push failed because the remote branch name is likely "master" instead of "main". I will try pushing to the "master" branch.


"""

!git push origin master

"""## Summary:

### Data Analysis Key Findings

*   Git was already installed in the Colab environment.
*   The GitHub repository "https://github.com/Kanchimunichenchureddy/UCI-Heart-Disease-Data.git" was successfully cloned, but it was initially empty.
*   Attempts to locate the current notebook file within the Colab environment's file system using standard shell commands (`ls`, `find`, `pwd`) were unsuccessful. The notebook content is not stored in a traditional file system location that can be directly copied.
*   Consequently, the tasks of copying the notebook file to the cloned repository directory, adding the notebook file to the git repository, and committing the changes failed due to the inability to access the file.
*   Pushing to both the `main` and `master` branches of the remote repository failed because there were no local commits to push, stemming from the inability to add and commit the notebook file in previous steps.

### Insights or Next Steps

*   Investigate alternative methods for saving or exporting the current Colab notebook content to a file that can be accessed and added to a Git repository.
*   Consider using the Colab interface's built-in "File" > "Download" option to save the notebook locally and then manually upload it to the cloned repository directory or use git commands from a local machine.

"""
